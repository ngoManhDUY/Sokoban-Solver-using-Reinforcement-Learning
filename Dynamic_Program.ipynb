{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1037,"status":"ok","timestamp":1696315153376,"user":{"displayName":"Duy Ngo Manh","userId":"11191220497016549307"},"user_tz":-420},"id":"Ou1q2LnRCPCr"},"outputs":[],"source":["import Sokoban_env\n","from Sokoban_env import Sokoban_v2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import rl_algorithms as rl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":338,"status":"error","timestamp":1696315156246,"user":{"displayName":"Duy Ngo Manh","userId":"11191220497016549307"},"user_tz":-420},"id":"plhvcJWdCPCv","outputId":"4ea3d32f-3da0-4dc4-c418-26070de984df"},"outputs":[],"source":["env = Sokoban_v2(map_name=\"level_1\")\n","env.reset()\n","plt.imshow(env.render())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.available_states(env)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["P = env.create_transition_pos()\n","print(P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def value_iteration(env,num_iterations = 100,threshold = 1e-20,gamma = 1.0):\n","    state_space = env.observation_space.n\n","    action_space = env.action_space.n\n","    value_table = np.zeros(state_space)\n","\n","    for i in range(num_iterations):\n","        update_value_table = np.copy(value_table)\n","\n","        for state in range(state_space):\n","            Q_values = []\n","            for action in range(action_space):\n","                transitions = env.P.get((state, action), [])  # Get possible transitions for (state, action)\n","                q_value = 0.0\n","                for next_state, prob in transitions:\n","                    reward = env.R.get((state, action, next_state), 0.0)  # Get the reward for the transition\n","                    q_value += prob * (reward + gamma * update_value_table[next_state])\n","                Q_values.append(q_value)\n","\n","            value_table[state] = max(Q_values)\n","\n","        if np.sum(np.fabs(update_value_table - value_table)) <= threshold:\n","            break\n","\n","    return value_table\n","\n","\n","def extract_policy(value_table, env):\n","    gamma = 1.0\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","\n","    for state in range(env.observation_space.n):\n","        Q_values = []\n","        for action in range(env.action_space.n):\n","            transitions = env.P.get((state, action), [])  # Get possible transitions for (state, action)\n","            q_value = 0.0\n","            for next_state, prob in transitions:\n","                reward = env.R.get((state, action, next_state), 0.0)  # Get the reward for the transition\n","                q_value += prob * (reward + gamma * value_table[next_state])\n","            Q_values.append(q_value)\n","\n","        policy[state] = np.argmax(Q_values)\n","\n","    return policy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimal_value_function = value_iteration(env)\n","optimal_policy = extract_policy(optimal_value_function,env)\n","print(optimal_policy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def policy_iteration(env, num_iterations=100, gamma=1.0):\n","    state_space = env.observation_space.n\n","    action_space = env.action_space.n\n","    policy = np.random.choice(action_space, size=state_space)\n","\n","    for _ in range(num_iterations):\n","        value_table = np.zeros(state_space)\n","        threshold = 1e-20\n","\n","        while True:\n","            update_value_table = np.copy(value_table)\n","            for state in range(state_space):\n","                action = policy[state]\n","                transitions = env.P.get((state, action), [])\n","                v = 0.0\n","                for next_state, prob in transitions:\n","                    reward = env.R.get((state, action, next_state), 0.0)\n","                    v += prob * (reward + gamma * update_value_table[next_state])\n","                value_table[state] = v\n","\n","            if np.sum(np.fabs(update_value_table - value_table)) <= threshold:\n","                break\n","\n","        # Policy Improvement\n","        policy_stable = True\n","        for state in range(state_space):\n","            old_action = policy[state]\n","            Q_values = np.zeros(action_space)\n","            for action in range(action_space):\n","                transitions = env.P.get((state, action), [])\n","                q_value = 0.0\n","                for next_state, prob in transitions:\n","                    reward = env.R.get((state, action, next_state), 0.0)\n","                    q_value += prob * (reward + gamma * value_table[next_state])\n","                Q_values[action] = q_value\n","\n","            best_action = np.argmax(Q_values)\n","            policy[state] = best_action\n","\n","            if old_action != best_action:\n","                policy_stable = False\n","\n","        if policy_stable:\n","            break\n","\n","    return policy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimal_policy = policy_iteration(env)\n","optimal_policy"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
